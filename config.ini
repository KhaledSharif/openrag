[main]
name=wikipedia

# define input documents from file, each line is URL to a wiki article, used when executing fetch.py
sources=sourcedocs.wiki.txt

# define evaluation questions from file, each line is a question used when executing eval.py
questions=questions.wiki.txt

# define where the output from evaluation questions (eval.py) is stored, will be overwritten
answers=answers.wiki.txt

# model to use for embedding & cosine similarity for RAG lookup
embedmodel=nomic-embed-text:latest

# model to use when executing fetch.py to extract useful text from output of html2text
chunkmodel=mistral:latest

# model to use when executing search.py (multi-hop single-question answering)
mainmodel=dolphin-llama3:latest

# models to use when executing eval.py (no-hop multi-question answering)
evalmodels=command-r:latest|dolphin-llama3:latest|mistral:latest

# number of documents to fetch on each hop
ndocs=5

# number of hops to perform when executing search.py
nhops=1

# number of top-ranking documents to keep on each hop
nhoptake=2

# max number of tokens to output from llm when asking question
npredict=1000

# number of tokens for llm context window when asking question
ncontext=4000

# number of chars in a single chunk input which is fed to chunkmodel
chunksize=30000