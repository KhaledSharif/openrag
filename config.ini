[main]
name=wikipedia

# define inputs & outputs
sources=sourcedocs.wiki.txt
questions=questions.wiki.txt
answers=answers.wiki.txt

# model to use for embedding & cosine similarity for RAG lookup
embedmodel=snowflake-arctic-embed:335m

# model to use when executing fetch.py
chunkmodel=dolphin-llama3:latest

# model to use when executing search.py (multi-hop single-question answering)
mainmodel=dolphin-llama3:latest

# models to use when executing eval.py (no-hop multi-question answering)
evalmodels=command-r:latest|dolphin-llama3:latest|mistral:latest

# number of documents to fetch on each hop
ndocs=5

# number of hops to perform when executing search.py
nhops=3

# number of top-ranking documents to keep on each hop
nhoptake=2

# max number of tokens to output from llm when asking question
npredict=1000

# number of tokens for llm context window when asking question
ncontext=4000

# number of chars in a single chunk input which is fed to chunkmodel
chunksize=40000