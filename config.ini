[main]
name=wikipedia

# define inputs & outputs
sources=sourcedocs.wiki.txt
questions=questions.wiki.txt
answers=answers.wiki.txt

# model to use for embedding & cosine similarity for RAG lookup
embedmodel=snowflake-arctic-embed:335m

# model to use when executing fetch.py
chunkmodel=mistral:latest

# model to use when executing search.py
mainmodel=dolphin-llama3:latest

# models to use when executing eval.py
evalmodels=command-r:latest|dolphin-llama3:latest|mistral:latest

# number of documents to fetch on each hop
ndocs=5

# number of hops to perform when executing search.py
nhops=3

# max number of tokens to output from llm when asking question
npredict=1000

# number of tokens for llm context window when asking question
ncontext=2000

# number of chars in a single chunk input which is fed to chunkmodel
chunksize=40000